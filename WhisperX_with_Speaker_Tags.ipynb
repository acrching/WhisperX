{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMVT2e8G4LaQGC9YQwyZoSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acrching/WhisperX/blob/main/WhisperX_with_Speaker_Tags.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the webcast link does not end in mp3 or mp4, download here using yt-dlp, else skip."
      ],
      "metadata": {
        "id": "PPGNZzAmquIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNVivZKzqnqw"
      },
      "outputs": [],
      "source": [
        "!pip install yt-dlp\n",
        "!yt-dlp #insert command from The Stream Detector here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install WhisperX"
      ],
      "metadata": {
        "id": "sKspHf3Cr0uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/m-bain/whisperx.git\n",
        "!pip install ctranslate2==4.4.0"
      ],
      "metadata": {
        "id": "Kez67EfGr-5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to transcribe!"
      ],
      "metadata": {
        "id": "0ahcByLQs_qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import gc\n",
        "\n",
        "device = \"cuda\"\n",
        "audio_file = \"audio.mp3\"  # you can directly paste the URL if mp3 or mp4\n",
        "batch_size = 32  # reduce if low on GPU mem\n",
        "compute_type = \"float16\"\n",
        "\n",
        "# Define your initial prompt with company-specific terms and financial terms\n",
        "company_specific_terms = \"These company-specific terms might be mentioned in the call: \" #add here company name, product names, and speaker names\n",
        "financial_terms = (\n",
        "    \"revenue, earnings, EBITDA, gross margin, net income, operating income, \"\n",
        "    \"cash flow, CapEx, OpEx, dividends, share buyback, guidance, fiscal year, \"\n",
        "    \"quarterly earnings, profit margin, cost of goods sold, EPS, diluted EPS, \"\n",
        "    \"working capital, debt, equity, assets, liabilities, return on investment, \"\n",
        "    \"operating expenses, net profit, free cash flow, balance sheet, income statement\"\n",
        ")\n",
        "\n",
        "initial_prompt_text = f\"{company_specific_terms}, {financial_terms}\"\n",
        "\n",
        "# 1. Transcribe with WhisperX, using the initial prompt parameter\n",
        "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, asr_options={\"initial_prompt\": initial_prompt_text})\n",
        "audio = whisperx.load_audio(audio_file)\n",
        "result = model.transcribe(audio, language='en')\n",
        "\n",
        "# Extract and save the transcript without timestamps and speaker tags\n",
        "transcript_text = \" \".join([segment['text'] for segment in result['segments']])\n",
        "\n",
        "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:  # you can change the text file name\n",
        "    f.write(transcript_text)\n",
        "\n",
        "print(\"Transcript saved\")\n",
        "\n",
        "print(result[\"segments\"])  # before alignment\n",
        "\n",
        "# 2. Align whisper output\n",
        "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "print(result[\"segments\"])  # after alignment\n",
        "\n",
        "# 3. Assign speaker labels\n",
        "diarize_model = whisperx.DiarizationPipeline(use_auth_token='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', device=device)\n",
        "diarize_segments = diarize_model(audio)\n",
        "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "\n",
        "current_speaker = None\n",
        "current_text = \"\"\n",
        "output_text = \"\"\n",
        "\n",
        "for segment in result[\"segments\"]:\n",
        "    # Handle segments without a speaker label\n",
        "    speaker = segment.get(\"speaker\", \"UNKNOWN\")  # Assign \"UNKNOWN\" if speaker is missing\n",
        "    if speaker != current_speaker:\n",
        "        if current_speaker:\n",
        "            output_text += f\"{current_speaker}^ {current_text.strip()}\\n\\n\"\n",
        "        current_speaker = speaker\n",
        "        current_text = \"\"\n",
        "    current_text += segment[\"text\"] + \" \"\n",
        "\n",
        "# Add the last speaker's text\n",
        "if current_speaker:\n",
        "    output_text += f\"{current_speaker}^ {current_text.strip()}\\n\"\n",
        "\n",
        "# Save to a text file, specifying UTF-8 encoding to handle Unicode characters\n",
        "output_file = \"output with speakers.txt\"  # you can change the text file name\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:  # Specify UTF-8 encoding\n",
        "    f.write(output_text)\n",
        "\n",
        "print(diarize_segments)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "As9tfclt_VFp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}